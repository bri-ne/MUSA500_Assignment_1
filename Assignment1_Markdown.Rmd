---
title: "Using OLS Regression to Predict Median House Values in Philadelphia" 
author: names 
date: date 
output: 
  html_document: 
    toc: true 
    toc_float: true 
    toc_depth: 6
    code_folding: "hide"
editor_options: 
  markdown: 
    wrap: 72
---

------------------------------------------------------------------------

-   

------------------------------------------------------------------------

```{r setup, include=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#--- load libraries ----


library(tidyr)
library(dplyr)
library(DAAG)
library(car)  #to calculate VIF
library(MASS)
library(rsq)
library(kableExtra)
library(tidyverse) #for ggplot
library(sf) #for maps
library(cowplot) #for plotgrid
library(classInt)#for jenks breaks
library(rgdal)
library(ggplot2)
library(RColorBrewer)

options(scipen=999)

#---- Step 1: Upload Data ----

ourdata <- read.csv("https://raw.githubusercontent.com/bri-ne/MUSA500_Assignment_1/main/RegressionData.csv")

```

#### **Introduction**

*intro paragraphs (2)*

#### **Methods**

###### **a. Data Cleaning**

Our study relies on census data, that includes observations for various
demographic variables at the census block group level. We began with a
data set of 1,816 observation and filtered out block groups with small
populations (\<40), block groups with no housing units, and block groups
with a median house value less than \$10,000. Additionally, a two
separate block groups in North Philadelphia were removed due to a very
high median house value(greater than \$80,000) and very low median
household income (less than \$8,000). We were left with 1,720
observation to use in our analysis.

###### **b. Exploratory Analysis**

To get a better grasp on our observations we calculated summary
statistics and examined the distribution of the variables. Below, Table
1 shows the summary statistics calculated for our dependent variable,
Median House Value (MEDHVAL) and our four independent variables:

-   Number of Households Living Below the Poverty Line (NBELPOV)

-   Percent of housing units that are detached single family houses
    (PCTSINGLES)

-   Percentof housing units that are vacant (PCTVACANT)

-   Percent of residents in Block Group with at least a bachelor's
    degree (PCTBACHMOR)

The distribution of our raw variables are shown in Figures 1. For the
sake of exploration we also illustrated the distribution of the natural
log of all our variables, as shown in Figure 2. From these
visualizations, it was clear that both the dependent variable and
NBELPOV were not normally distrubted. To account for this, we concluded
that it would be best to use the natural logged dependent variable and
NBELPOV. We used the raw versions of the remaining variables.

```{r summarydist_code, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE}

#### THIS FIRST CHUNK INCLUDES CODE FOR THE SUMMARY STATISTICS AND THE HISTOGRAMS ####

#save the summary to a df (i have 0 idea how this code works, just a copy paste from the web)
#we really only need the mean and sd, so I will clean this up
summary <-as.data.frame(apply(ourdata,2,summary))
#checking the index
base::row(summary,1)
#dropping all rows except the mean
summary<-summary[-c(1,2,3,5,6),]
#transposing so variables (mean and sd) are columns
summary <- summary%>%t()%>%as.data.frame()

#getting sd and adding sd column
sdMedV <- sd(ourdata$MEDHVAL)
sdBach <- sd(ourdata$PCTBACHMOR)
sdNbel <- sd(ourdata$NBELPOV100)
sdVac <- sd(ourdata$PCTVACANT)
sdSing <- sd(ourdata$PCTSINGLES)
sdMedHHINC <- sd(ourdata$MEDHHINC)

sdcol <- list(0, 0, sdMedV, sdBach, sdMedHHINC, sdVac, sdSing, sdNbel)
summary$sd <- sdcol

#dropping the other things we don't need 
base::row(summary,1)
summary<- summary[-c(1,2,5),]
summary<- summary%>%as.data.frame()
summary


#create a copy of the summary table to give it nicer row names for the table below
summary_nice_names <- summary
rownames(summary_nice_names) <- c("Median House Value",
                                  "% of Indviduals with Bachelor's Degrees or Higher",
                                  '% of Vacant Houses',
                                  "% of Single House Units",
                                  "% Households Living in Poverty")
#final summary stats table
summary_table <- kable(summary_nice_names) %>%
  kable_styling() %>%
  footnote(general_title = "Summary Statistics\n",
           general = "Table 1")


#---- Step 1A.ii: Histograms ----

#check them out, regular histogram

 
hists <- histogram( ~ MEDHVAL +PCTBACHMOR +NBELPOV100 +PCTSINGLES +PCTVACANT, layout=c(2,3), data = ourdata, main='Distribution of Raw Variables', sub= 'Figure 1', col="#8C96C6", breaks = 50, scales='free')   

#none look normal so we will examine if a log transformation will make them normal
#remember to use 1+ if any variable has zero values
#the only variable that does not have a 0 value is MEDHVAL

ourdata$LNMEDHVAL <- log(ourdata$MEDHVAL)

ourdata$LNPCTBACHMOR <- log(1+ourdata$PCTBACHMOR)

ourdata$LNBELPOV100 <- log(1+ourdata$NBELPOV100)

ourdata$LNPCTVACANT <- log(1+ourdata$PCTVACANT)

ourdata$LNPCTSINGLES <- log(1+ourdata$PCTSINGLES)


#### Logged 

LNhists <- histogram( ~ LNMEDHVAL +LNPCTBACHMOR +LNBELPOV100 +LNPCTSINGLES +LNPCTVACANT,layout=c(2,3),data = ourdata, 
                    main='Distribution of Natural Log of Variables', sub= 'Figure 2', col="#B3CDE3", breaks = 50, scales='free')  

```

```{r summary_output,  echo=FALSE, warning=FALSE, message=FALSE}

summary_table
hists
LNhists


```

To further explore out data, we wanted to determine if any of our
independent, or predictor, variables were correlated with our dependent
variable.

If two of our independent or predictor variables were correlated with
each other, we would remove one of them. Collinear independent variable
s

```{r correlation_code, message=FALSE, echo=FALSE, warning=FALSE, include=FALSE}
#---- Step 1B: Variable Scatter Plots ----
#investigate to see if our predictors relationship with the dependent variable
# is linear by plotting as scatter plots

pLNBelpov <- ggplot(ourdata, aes(x = LNMEDHVAL, y= LNBELPOV100))+
            geom_point(size=2.5, color = "#4D004B", alpha = 0.5)+theme_minimal()
            
pBach <- ggplot(ourdata, aes(x = LNMEDHVAL, y= PCTBACHMOR))+
  geom_point(size=2.5, color = "#4D004B", alpha = 0.5)+theme_minimal()
  
  
pVac <- ggplot(ourdata, aes(x = LNMEDHVAL, y= PCTVACANT))+
  geom_point(size=2.5, color = "#4D004B", alpha = 0.5)+theme_minimal()
  
  

pSing <- ggplot(ourdata, aes(x = LNMEDHVAL, y= PCTSINGLES))+
  geom_point(size=2.5, color = "#4D004B", alpha = 0.5)+theme_minimal()



scattergrid <- plot_grid( 
           pLNBelpov, 
           pBach, 
           pVac, 
           pSing, 
           labels = c("% Below Poverty (LN)", 
                      "Bachelors Degree",
                      "% Vacant Homes", 
                      "% Single House Units"), 
           label_x =-.1,
           label_y = 1.03,
           scale= 0.9,
           align = 'hv',
           ncol = 2, nrow = 2)



```

```{r correlation_output, warning=FALSE, message=FALSE, echo=TRUE, fig.dim = c(8, 12)}
scattergrid

```

###### **c. Multiple Regression Analysis**

This analysis uses multiple Ordinary Least Squares (OLS) regression to
examine the relationship between our variable of interest and our
explanatory variables. OLS multiple regression determines the strength
and direction (positive, negative, zero) of the relationship, and
goodness of model fit. We calculate the coefficient 𝛽i of each predictor
which is interpreted as the amount by which the dependent variable, in
our case median house value, changes as the independent variable
increases by one unit, holding all other predictors constant. The model
includes the error term 𝜀, and is defined, for each observation i, as
the vertical distance from the observed value and the predicted value of
y. The error term is included in the equation to allow a point to fall
either above or below the regression line. We are regressing median
house value on percentage of homes living below poverty, percentage of
individuals with a bachelor's degree or higher, percentage of vacant
homes, and percentage of single house units in Philadelphia. Our
regression equation is as follows:

$$y = MEDHVAL = \beta_{0} + \beta_{1}LNBELPOV100 + \beta_{2}PCTBACHMOR + beta_{3}PCTVACANT + \beta_{4}PCTSINGLES + \epsilon$$

OLS multiple regression models hold assumptions, the first being a
linear relationship between x and y, which can be teste via scatter
plot. The second assumption is normality of residuals. This is important
for point estimation, confidence intervals, and hypothesis tests for
small samples due to the Central Limit Theorem. Normality is essential
for all sample sizes in order to predict future values of y. If
residuals appear to have a non-normal distribution, it may indicate a
non-linear relationship, or non-normal distributions of the
dependent/independent variables. This may be solved with a logarithmic
transformation of the variables. The third and fourth assumptions are
that residuals are random and homoscedastic. There should be no
relationship or pattern between the observed values and the predicted
values. Homoscedasticity means the variance of residuals should look
constant for different values when plotted. The fifth assumption states
that observations and residuals must be independent. If data has a
temporal or spatial component, residuals of observations that are close
in time or space will be autocorrelated, or dependent on one another. In
this case, time series or spatial regression should be used over OLS.
The Final assumption of multiple OLS regression is the absence of
multicollinearity. Said differently, the predictors should not be
strongly correlated with one another. The presence of multicollinearity
causes unstable coefficient estimates and weakens the model. The
parameters of multiple regression are coefficients β0 ,..., βk, where k
= number of predictors. β0 represents the y- intercept of the regression
line. Β1 ... βk represent the coefficients of variables x1 ... xk. Each
independent variable will have its own slope coefficient which will
indicate the relationship of that predictor with the dependent variable,
controlling for all other independent variables in the regression. The
function of OLS regression is to calculate the sum of least squares of
residuals. The least squares estimates for β ̂\_0,..., β ̂\_k are
obtained when the quantity for SSE (equation below) is minimized. $$
SSE = \sum_{i=1}^{n} \epsilon^2 = \sum_{i=1}^{n} (y-\hat{y})^2    
= \sum_{i=1}^{n}(y_{i}- \hat{\beta}_{0}- \hat{\beta}_{1}x_{1i}- \hat{\beta}_{2}x_{2i}- ...- \hat{\beta}_{k}x_{ki})^2
$$

Variance is the other parameter that needs to be estimated in multiple
regression, calculated as $σ^2=\frac{SSE}{(n-(k+1))}=MSE$ , where k =
number of predictors, n = number of observations, and MSE stands Mean
Squared Error. In multiple regression, R2 is the coefficient of multiple
determination, or the proportion of variance in the model explained by
all k predictors, represented by the equation $R^2=1-\frac{SSE}{SST}$ .
The R2 will increase with more predictors in the model and can be
adjusted for the number of predictors with the equation
$R_{adj}^2=\frac{(n-1) R^2-k}{n-(k+1)}$ The model utility test, F-ratio,
is conducted on the regression model to determine a goodness of fit
measure. It can be interpreted as a significance test for R2. The
F-ratio tests the null hypothesis, H0 that all coefficients in the model
are jointly zero, vs the alternative hypothesis Ha that at least one of
the coefficients is not 0. Said differently, we test to make sure none
of the independent variables is a significant predictor o the dependent
variable. From the F-ratio we are looking for a P value that is less
than 0.05. Once an F-ratio is determined, we run a T-test on every
individual predictor. The null hypothesis states that the predictor (ie;
Percentage of vacant homes) has no association with the dependent
variable, once again in our case is median house value. Our goal is to
reject the null hypothesis H0 in favor of the alternative hypothesis Ha
which is Βi ≠ 0 for each predictor.

###### **d. Additional Analysis**

#### **Results**

###### **a. Exploratory Results**

###### **b. Regression Results**

###### **c. Regression Assumption Checks**

###### **d. Additional Models**

#### **Limitations**
