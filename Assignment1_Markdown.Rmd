---

-title: "Using OLS Regression to Predict Median House Values in Philadelphia"
author: names
date: date
output: html_document
toc: true
toc_float: true
code_folding: "hide"

---
  
    
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



#### **Introduction**
  
*intro paragraphs (2)*  

#### **Methods**

###### **a. Data Cleaning**  

###### **b. Exploratory Analysis**  

###### **c. Multiple Regression Analysis** 
This analysis uses multiple Ordinary Least Squares (OLS) regression to examine the relationship between our variable of interest and our explanatory variables. OLS multiple regression determines the strength and direction (positive, negative, zero) of the relationship, and goodness of model fit. We calculate the coefficient ùõΩi of each predictor which is interpreted as the amount by which the dependent variable, in our case median house value, changes as the independent variable increases by one unit, holding all other predictors constant. The model includes the error term ùúÄ, and is defined, for each observation i, as the vertical distance from the observed value and the predicted value of y. The error term is included in the equation to allow a point to fall either above or below the regression line. We are regressing median house value on percentage of homes living below poverty, percentage of individuals with a bachelor's degree or higher, percentage of vacant homes, and percentage of single house units in Philadelphia. Our regression equation is as follows:

$$y = MEDHVAL = \beta_{0} + \beta_{1}LNBELPOV100 + \beta_{2}PCTBACHMOR + beta_{3}PCTVACANT + \beta_{4}PCTSINGLES + \epsilon$$

OLS multiple regression models hold assumptions, the first being a linear relationship between x and y, which can be teste via scatter plot. The second assumption is normality of residuals. This is important for point estimation, confidence intervals, and hypothesis tests for small samples due to the Central Limit Theorem. Normality is essential for all sample sizes in order to predict future values of y. If residuals appear to have a non-normal distribution, it may indicate a non-linear relationship, or non-normal distributions of the dependent/independent variables. This may be solved with a logarithmic transformation of the variables. The third and fourth assumptions are that residuals are random and homoscedastic. There should be no relationship or pattern between the observed values and the predicted values. Homoscedasticity means the variance of residuals should look constant for different values when plotted. The fifth assumption states that observations and residuals must be independent. If data has a temporal or spatial component, residuals of observations that are close in time or space will be autocorrelated, or dependent on one another. In this case, time series or spatial regression should be used over OLS. The Final assumption of multiple OLS regression is the absence of multicollinearity. Said differently, the predictors should not be strongly correlated with one another. The presence of multicollinearity causes unstable coefficient estimates and weakens the model. 
The parameters of multiple regression are coefficients Œ≤0 ,..., Œ≤k, where k = number of predictors. Œ≤0 represents the y- intercept of the regression line. Œí1 ‚Ä¶ Œ≤k represent the coefficients of variables x1 ‚Ä¶ xk. Each independent variable will have its own slope coefficient which will indicate the relationship of that predictor with the dependent variable, controlling for all other independent variables in the regression. The function of OLS regression is to calculate the sum of least squares of residuals. The least squares estimates for Œ≤ ÃÇ_0,‚Ä¶, Œ≤ ÃÇ_k are obtained when the quantity for SSE (equation below) is minimized. 
$$
SSE = \sum_{i=1}^{n} \epsilon^2 = \sum_{i=1}^{n} (y-\hat{y})^2    
= \sum_{i=1}^{n}(y_{i}- \hat{\beta}_{0}- \hat{\beta}_{1}x_{1i}- \hat{\beta}_{2}x_{2i}- ...- \hat{\beta}_{k}x_{ki})^2
$$

Variance is the other parameter that needs to be estimated in multiple regression, calculated as
 $œÉ^2=\frac{SSE}{(n-(k+1))}=MSE$
 , where k = number of predictors, n = number of observations, and MSE stands Mean Squared Error. In multiple regression, R2 is the coefficient of multiple determination, or the proportion of variance in the model explained by all k predictors, represented by the equation
 $R^2=1-\frac{SSE}{SST}$ . The R2 will increase with more predictors in the model and can be adjusted for the number of predictors with the equation $R_{adj}^2=\frac{(n-1) R^2-k}{n-(k+1)}$ 
	The model utility test, F-ratio, is conducted on the regression model to determine a goodness of fit measure. It can be interpreted as a significance test for R2. The F-ratio tests the null hypothesis, H0 that all coefficients in the model are jointly zero, vs the alternative hypothesis Ha that at least one of the coefficients is not 0. Said differently, we test to make sure none of the independent variables is a significant predictor o the dependent variable. From the F-ratio we are looking for a P value that is less than 0.05. Once an F-ratio is determined, we run a T-test on every individual predictor. The null hypothesis states that the predictor (ie; Percentage of vacant homes) has no association with the dependent variable, once again in our case is median house value. Our goal is to reject the null hypothesis H0 in favor of the alternative hypothesis Ha which is Œíi ‚â† 0 for each predictor. 

###### **d. Additional Analysis**  
  
#### **Results**  

###### **a. Exploratory Results**  

###### **b. Regression Results**  

###### **c. Regression Assumption Checks**  

###### **d. Additional Models**  

#### **Limitations**

